{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1c72d1a-517a-4a37-b2ce-50d94aa90d0f",
   "metadata": {},
   "source": [
    "# Phase 3: Data Preprocessing & Feature Engineering\n",
    "\n",
    "- Load raw train data\n",
    "- Clean and transform\n",
    "- Engineer key features (time-based, frequency, distance, amount, age)\n",
    "- Handle PII ethically\n",
    "- Save processed data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ab5068-8891-48f5-8898-9e112ee58261",
   "metadata": {},
   "source": [
    "## Imports & Path Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7dc3f35-d2e4-45cd-a910-f39c0ebd413e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using file: ../data/raw/fraudTrain.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "\n",
    "RAW_DATA_PATH = '../data/raw/'\n",
    "PROCESSED_DATA_PATH = '../data/processed/'\n",
    "\n",
    "os.makedirs(PROCESSED_DATA_PATH, exist_ok=True)\n",
    "\n",
    "train_file = os.path.join(RAW_DATA_PATH, 'fraudTrain.csv')\n",
    "sample_file = os.path.join(RAW_DATA_PATH, 'fraudTrain_sample_10k.csv')\n",
    "\n",
    "# For faster development: use sample first, then switch to full train\n",
    "USE_SAMPLE = False\n",
    "data_file = sample_file if USE_SAMPLE else train_file\n",
    "\n",
    "print(\"Using file:\", data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ffe2c6-27af-46fc-95de-8654353034e6",
   "metadata": {},
   "source": [
    "## Load data & Initial cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb2142a2-9f57-4b66-aa00-8289732aa5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (1296675, 22)\n",
      "\n",
      "Columns: ['trans_date_trans_time', 'cc_num', 'merchant', 'category', 'amt', 'first', 'last', 'gender', 'street', 'city', 'state', 'zip', 'lat', 'long', 'city_pop', 'job', 'dob', 'trans_num', 'unix_time', 'merch_lat', 'merch_long', 'is_fraud']\n",
      "\n",
      "Missing values:\n",
      " trans_date_trans_time    0\n",
      "cc_num                   0\n",
      "merchant                 0\n",
      "category                 0\n",
      "amt                      0\n",
      "first                    0\n",
      "last                     0\n",
      "gender                   0\n",
      "street                   0\n",
      "city                     0\n",
      "state                    0\n",
      "zip                      0\n",
      "lat                      0\n",
      "long                     0\n",
      "city_pop                 0\n",
      "job                      0\n",
      "dob                      0\n",
      "trans_num                0\n",
      "unix_time                0\n",
      "merch_lat                0\n",
      "merch_long               0\n",
      "is_fraud                 0\n",
      "dtype: int64\n",
      "\n",
      "After cleaning shape: (1296675, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trans_date_trans_time</th>\n",
       "      <th>cc_num</th>\n",
       "      <th>merchant</th>\n",
       "      <th>category</th>\n",
       "      <th>amt</th>\n",
       "      <th>gender</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>city_pop</th>\n",
       "      <th>dob</th>\n",
       "      <th>merch_lat</th>\n",
       "      <th>merch_long</th>\n",
       "      <th>is_fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-01 00:00:18</td>\n",
       "      <td>2703186189652095</td>\n",
       "      <td>fraud_Rippin, Kub and Mann</td>\n",
       "      <td>misc_net</td>\n",
       "      <td>4.97</td>\n",
       "      <td>F</td>\n",
       "      <td>36.0788</td>\n",
       "      <td>-81.1781</td>\n",
       "      <td>3495</td>\n",
       "      <td>1988-03-09</td>\n",
       "      <td>36.011293</td>\n",
       "      <td>-82.048315</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-01 00:00:44</td>\n",
       "      <td>630423337322</td>\n",
       "      <td>fraud_Heller, Gutmann and Zieme</td>\n",
       "      <td>grocery_pos</td>\n",
       "      <td>107.23</td>\n",
       "      <td>F</td>\n",
       "      <td>48.8878</td>\n",
       "      <td>-118.2105</td>\n",
       "      <td>149</td>\n",
       "      <td>1978-06-21</td>\n",
       "      <td>49.159047</td>\n",
       "      <td>-118.186462</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-01 00:00:51</td>\n",
       "      <td>38859492057661</td>\n",
       "      <td>fraud_Lind-Buckridge</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>220.11</td>\n",
       "      <td>M</td>\n",
       "      <td>42.1808</td>\n",
       "      <td>-112.2620</td>\n",
       "      <td>4154</td>\n",
       "      <td>1962-01-19</td>\n",
       "      <td>43.150704</td>\n",
       "      <td>-112.154481</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  trans_date_trans_time            cc_num                         merchant  \\\n",
       "0   2019-01-01 00:00:18  2703186189652095       fraud_Rippin, Kub and Mann   \n",
       "1   2019-01-01 00:00:44      630423337322  fraud_Heller, Gutmann and Zieme   \n",
       "2   2019-01-01 00:00:51    38859492057661             fraud_Lind-Buckridge   \n",
       "\n",
       "        category     amt gender      lat      long  city_pop        dob  \\\n",
       "0       misc_net    4.97      F  36.0788  -81.1781      3495 1988-03-09   \n",
       "1    grocery_pos  107.23      F  48.8878 -118.2105       149 1978-06-21   \n",
       "2  entertainment  220.11      M  42.1808 -112.2620      4154 1962-01-19   \n",
       "\n",
       "   merch_lat  merch_long  is_fraud  \n",
       "0  36.011293  -82.048315         0  \n",
       "1  49.159047 -118.186462         0  \n",
       "2  43.150704 -112.154481         0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the CSV data into a DataFrame (skip unnamed index if present)\n",
    "df = pd.read_csv(data_file, index_col=0)\n",
    "\n",
    "# Print basic info for verification\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"\\nColumns:\", df.columns.tolist())\n",
    "print(\"\\nMissing values:\\n\", df.isnull().sum())\n",
    "\n",
    "# Convert transaction time column to datetime for easier extraction (e.g., hour, month)\n",
    "df['trans_date_trans_time'] = pd.to_datetime(df['trans_date_trans_time'], errors='coerce')\n",
    "\n",
    "# Convert date of birth to datetime for age calculation\n",
    "df['dob'] = pd.to_datetime(df['dob'], errors='coerce')\n",
    "\n",
    "# Drop any rows with bad dates (should be 0 or few)\n",
    "df = df.dropna(subset=['trans_date_trans_time'])\n",
    "\n",
    "# Drop unnecessary/redundant columns (e.g., unique IDs not useful for modeling)\n",
    "cols_to_drop = ['trans_num', 'unix_time']\n",
    "df = df.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "# Ethical handling: Drop direct PII columns to avoid privacy risks; we'll compute age but consider excluding it from models\n",
    "pii_cols = ['first', 'last', 'street', 'city', 'state', 'zip', 'job']\n",
    "df = df.drop(columns=pii_cols, errors='ignore')\n",
    "\n",
    "print(\"\\nAfter cleaning shape:\", df.shape)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6733f09-abec-4b2c-a086-b3e25540ac11",
   "metadata": {},
   "source": [
    "## Basic time features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0679ecf1-a4bd-4ff8-be80-f77bca6fa518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract hour of day from transaction time (for patterns like overnight fraud spikes)\n",
    "df['trans_hour'] = df['trans_date_trans_time'].dt.hour\n",
    "\n",
    "# Extract day of week (0=Monday, 6=Sunday; for weekend patterns)\n",
    "df['trans_dayofweek'] = df['trans_date_trans_time'].dt.dayofweek\n",
    "\n",
    "# Extract month (for January/holiday seasonality; fraud spikes in Jan post-holidays)\n",
    "df['trans_month'] = df['trans_date_trans_time'].dt.month\n",
    "\n",
    "# Research-inspired: Flag late-night hours (midnight-3am, where fraud often spikes due to low monitoring)\n",
    "df['is_late_night'] = df['trans_hour'].apply(lambda x: 1 if (0 <= x <= 3) else 0)\n",
    "\n",
    "# Broader night flag (21:00-05:00, as fallback)\n",
    "df['is_night'] = df['trans_hour'].apply(lambda x: 1 if (x >= 21 or x < 4) else 0)\n",
    "\n",
    "# Holiday season flag (Nov-Dec for Black Friday/Cyber Monday spikes)\n",
    "df['is_holiday_season'] = df['trans_month'].apply(lambda x: 1 if x in [11, 12] else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c90f3b-bdc9-4edd-861d-19c5ca875b39",
   "metadata": {},
   "source": [
    "## Distance between user & merchant (Strong fraud signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65eb0c4e-7b44-4f3d-9b69-d7812fac291e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    1.296675e+06\n",
      "mean     7.611465e+01\n",
      "std      2.911693e+01\n",
      "min      2.225452e-02\n",
      "25%      5.533491e+01\n",
      "50%      7.823175e+01\n",
      "75%      9.850327e+01\n",
      "max      1.521172e+02\n",
      "Name: distance_km, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Haversine function to calculate km distance (accounts for Earth's curve; fraud often far from user location)\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371  # Earth radius in km\n",
    "    dlat = radians(lat2 - lat1)\n",
    "    dlon = radians(lon2 - lon1)\n",
    "    a = sin(dlat/2)**2 + cos(radians(lat1)) * cos(radians(lat2)) * sin(dlon/2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1-a))\n",
    "    return R * c\n",
    "\n",
    "# Apply to each row to create distance feature\n",
    "df['distance_km'] = df.apply(\n",
    "    lambda row: haversine(\n",
    "        row['lat'], row['long'],\n",
    "        row['merch_lat'], row['merch_long']\n",
    "    ), axis=1\n",
    ")\n",
    "\n",
    "# Quick stats to verify (distances should range 0-1000+ km)\n",
    "print(df['distance_km'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7ebe32-f9e2-4a82-a892-c0b86781f5be",
   "metadata": {},
   "source": [
    "## Time-based frequency & recency per card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f693f1a-eca1-41ae-9f6e-3f8498eba58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ph/hmdftnmd4hb5nmchd4g2mgmm0000gp/T/ipykernel_2101/320460016.py:31: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby('cc_num', group_keys=False).apply(last1DayTransactionCount)\n",
      "/var/folders/ph/hmdftnmd4hb5nmchd4g2mgmm0000gp/T/ipykernel_2101/320460016.py:32: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby('cc_num', group_keys=False).apply(last7DaysTransactionCount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           cc_num trans_date_trans_time  time_since_last_trans  count_1_day  \\\n",
      "1017  60416207185   2019-01-01 12:47:15               0.000000          0.0   \n",
      "2724  60416207185   2019-01-02 08:44:57              19.961667          1.0   \n",
      "2726  60416207185   2019-01-02 08:47:36               0.044167          2.0   \n",
      "2882  60416207185   2019-01-02 12:38:14               3.843889          3.0   \n",
      "2907  60416207185   2019-01-02 13:10:46               0.542222          3.0   \n",
      "4135  60416207185   2019-01-03 13:56:35              24.763611          0.0   \n",
      "4337  60416207185   2019-01-03 17:05:10               3.143056          1.0   \n",
      "5467  60416207185   2019-01-04 13:59:55              20.912500          1.0   \n",
      "6027  60416207185   2019-01-04 21:17:22               7.290833          1.0   \n",
      "6273  60416207185   2019-01-05 00:42:24               3.417222          2.0   \n",
      "\n",
      "      count_7_days  count_30_days  \n",
      "1017           0.0            0.0  \n",
      "2724           1.0            1.0  \n",
      "2726           2.0            2.0  \n",
      "2882           3.0            3.0  \n",
      "2907           4.0            4.0  \n",
      "4135           5.0            5.0  \n",
      "4337           6.0            6.0  \n",
      "5467           7.0            7.0  \n",
      "6027           8.0            8.0  \n",
      "6273           9.0            9.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ph/hmdftnmd4hb5nmchd4g2mgmm0000gp/T/ipykernel_2101/320460016.py:33: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby('cc_num', group_keys=False).apply(last30DaysTransactionCount)\n"
     ]
    }
   ],
   "source": [
    "# Sort by card number and transaction time (required for groupby diff and rolling)\n",
    "df = df.sort_values(['cc_num', 'trans_date_trans_time'])\n",
    "\n",
    "# Time since last transaction (in hours)\n",
    "df['time_since_last_trans'] = df.groupby('cc_num')['trans_date_trans_time'].diff().dt.total_seconds() / 3600\n",
    "df['time_since_last_trans'] = df['time_since_last_trans'].fillna(0)\n",
    "\n",
    "# Creator's functions for rolling counts (exact count in last N days, excluding current)\n",
    "def last1DayTransactionCount(group):\n",
    "    temp = pd.Series(group.index, index=group['trans_date_trans_time'], name='count_1_day').sort_index()\n",
    "    count_1_day = temp.rolling('1d').count() - 1\n",
    "    count_1_day.index = temp.values\n",
    "    group['count_1_day'] = count_1_day.reindex(group.index)\n",
    "    return group\n",
    "\n",
    "def last7DaysTransactionCount(group):\n",
    "    temp = pd.Series(group.index, index=group['trans_date_trans_time'], name='count_7_days').sort_index()\n",
    "    count_7_days = temp.rolling('7d').count() - 1\n",
    "    count_7_days.index = temp.values\n",
    "    group['count_7_days'] = count_7_days.reindex(group.index)\n",
    "    return group\n",
    "\n",
    "def last30DaysTransactionCount(group):\n",
    "    temp = pd.Series(group.index, index=group['trans_date_trans_time'], name='count_30_days').sort_index()\n",
    "    count_30_days = temp.rolling('30d').count() - 1\n",
    "    count_30_days.index = temp.values\n",
    "    group['count_30_days'] = count_30_days.reindex(group.index)\n",
    "    return group\n",
    "\n",
    "# Apply the functions per group (slow on full data — run once, ~1–2 min)\n",
    "df = df.groupby('cc_num', group_keys=False).apply(last1DayTransactionCount)\n",
    "df = df.groupby('cc_num', group_keys=False).apply(last7DaysTransactionCount)\n",
    "df = df.groupby('cc_num', group_keys=False).apply(last30DaysTransactionCount)\n",
    "\n",
    "# Quick check\n",
    "print(df[['cc_num', 'trans_date_trans_time', 'time_since_last_trans', \n",
    "          'count_1_day', 'count_7_days', 'count_30_days']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9120dd-8a2b-4c3e-bee4-af92d2245f22",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0e76d33-ae88-4992-81fc-b620022e09e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           cc_num  merch_lat  merch_long  usual_lat  usual_long  \\\n",
      "1017  60416207185  43.974711 -109.741904  43.974711 -109.741904   \n",
      "2724  60416207185  42.018766 -109.044172  43.974711 -109.741904   \n",
      "2726  60416207185  42.961335 -109.157564  42.996738 -109.393038   \n",
      "2882  60416207185  42.228227 -108.747683  42.984937 -109.314547   \n",
      "2907  60416207185  43.321745 -108.091143  42.795760 -109.172831   \n",
      "4135  60416207185  43.477317 -109.467136  42.900957 -108.956493   \n",
      "4337  60416207185  42.871477 -109.160268  42.801478 -108.901540   \n",
      "5467  60416207185  43.332599 -108.318444  42.972020 -108.924759   \n",
      "6027  60416207185  43.598123 -108.977767  43.046273 -108.756935   \n",
      "6273  60416207185  42.314401 -108.554520  43.320252 -108.802952   \n",
      "\n",
      "      distance_from_usual_km  \n",
      "1017                0.000000  \n",
      "2724              224.769219  \n",
      "2726               19.556262  \n",
      "2882               96.083975  \n",
      "2907              105.563313  \n",
      "4135               76.296273  \n",
      "4337               22.486494  \n",
      "5467               63.456121  \n",
      "6027               63.910326  \n",
      "6273              113.666104  \n"
     ]
    }
   ],
   "source": [
    "# New feature: Distance from \"usual\" location (mean lat/long of last 5 transactions per card)\n",
    "# Calculate rolling mean lat/long per card\n",
    "df['usual_lat'] = df.groupby('cc_num')['merch_lat'].rolling(window=5, min_periods=1).mean().shift(1).reset_index(level=0, drop=True)\n",
    "df['usual_long'] = df.groupby('cc_num')['merch_long'].rolling(window=5, min_periods=1).mean().shift(1).reset_index(level=0, drop=True)\n",
    "df['usual_lat'] = df['usual_lat'].fillna(df['merch_lat'])  # for first transactions, use current\n",
    "df['usual_long'] = df['usual_long'].fillna(df['merch_long'])\n",
    "\n",
    "# Calculate distance from usual location (using haversine function)\n",
    "df['distance_from_usual_km'] = df.apply(\n",
    "    lambda row: haversine(row['usual_lat'], row['usual_long'], row['merch_lat'], row['merch_long']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Quick check\n",
    "print(df[['cc_num', 'merch_lat', 'merch_long', 'usual_lat', 'usual_long', 'distance_from_usual_km']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475286da-c845-4c08-8337-b5aa184c845f",
   "metadata": {},
   "source": [
    "## Amount Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac431d09-4153-4d41-b2c9-ae5eae4dfcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log transform amount (handles skewness; fraud often in extreme amounts)\n",
    "df['log_amt'] = np.log1p(df['amt'])  # log(1 + amt) to avoid log(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634b76d7-6954-452d-be69-b4b2d072344b",
   "metadata": {},
   "source": [
    "## Age features based on research (targetted groups between 30 and 39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aea65694-68fc-41ed-ae05-4e4af85ef517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approximate age at transaction (year difference; for patterns like senior targeting)\n",
    "trans_year = df['trans_date_trans_time'].dt.year\n",
    "birth_year = df['dob'].dt.year\n",
    "df['age_at_trans'] = trans_year - birth_year\n",
    "\n",
    "# Bin age for groups (seniors 60+ for higher loss insights)\n",
    "bins = [0, 20, 30, 40, 50, 60, 120]\n",
    "labels = ['<20', '20-29', '30-39', '40-49', '50-59', '60+']\n",
    "df['age_bin'] = pd.cut(df['age_at_trans'], bins=bins, labels=labels)\n",
    "\n",
    "# Optional flag based on stats for 30-39 (highest volume group)\n",
    "# df['is_30_39'] = df['age_bin'].apply(lambda x: 1 if x == '30-39' else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2946511-282b-442d-9687-280718fd2cbf",
   "metadata": {},
   "source": [
    "## Reorder columns & save processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d3b44f8-eec9-43b4-a759-77f0d4f42267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved to: ../data/processed/train_processed.csv\n",
      "\n",
      "Processed shape: (1296675, 28)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trans_date_trans_time</th>\n",
       "      <th>category</th>\n",
       "      <th>amt</th>\n",
       "      <th>log_amt</th>\n",
       "      <th>trans_hour</th>\n",
       "      <th>is_late_night</th>\n",
       "      <th>trans_month</th>\n",
       "      <th>is_holiday_season</th>\n",
       "      <th>distance_km</th>\n",
       "      <th>time_since_last_trans</th>\n",
       "      <th>...</th>\n",
       "      <th>long</th>\n",
       "      <th>city_pop</th>\n",
       "      <th>dob</th>\n",
       "      <th>merch_lat</th>\n",
       "      <th>merch_long</th>\n",
       "      <th>trans_dayofweek</th>\n",
       "      <th>is_night</th>\n",
       "      <th>usual_lat</th>\n",
       "      <th>usual_long</th>\n",
       "      <th>distance_from_usual_km</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1017</th>\n",
       "      <td>2019-01-01 12:47:15</td>\n",
       "      <td>misc_net</td>\n",
       "      <td>7.27</td>\n",
       "      <td>2.112635</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>127.606239</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-108.8964</td>\n",
       "      <td>1645</td>\n",
       "      <td>1986-02-17</td>\n",
       "      <td>43.974711</td>\n",
       "      <td>-109.741904</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>43.974711</td>\n",
       "      <td>-109.741904</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2724</th>\n",
       "      <td>2019-01-02 08:44:57</td>\n",
       "      <td>gas_transport</td>\n",
       "      <td>52.94</td>\n",
       "      <td>3.987872</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>110.308921</td>\n",
       "      <td>19.961667</td>\n",
       "      <td>...</td>\n",
       "      <td>-108.8964</td>\n",
       "      <td>1645</td>\n",
       "      <td>1986-02-17</td>\n",
       "      <td>42.018766</td>\n",
       "      <td>-109.044172</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>43.974711</td>\n",
       "      <td>-109.741904</td>\n",
       "      <td>224.769219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2726</th>\n",
       "      <td>2019-01-02 08:47:36</td>\n",
       "      <td>gas_transport</td>\n",
       "      <td>82.08</td>\n",
       "      <td>4.419804</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>21.787261</td>\n",
       "      <td>0.044167</td>\n",
       "      <td>...</td>\n",
       "      <td>-108.8964</td>\n",
       "      <td>1645</td>\n",
       "      <td>1986-02-17</td>\n",
       "      <td>42.961335</td>\n",
       "      <td>-109.157564</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>42.996738</td>\n",
       "      <td>-109.393038</td>\n",
       "      <td>19.556262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     trans_date_trans_time       category    amt   log_amt  trans_hour  \\\n",
       "1017   2019-01-01 12:47:15       misc_net   7.27  2.112635          12   \n",
       "2724   2019-01-02 08:44:57  gas_transport  52.94  3.987872           8   \n",
       "2726   2019-01-02 08:47:36  gas_transport  82.08  4.419804           8   \n",
       "\n",
       "      is_late_night  trans_month  is_holiday_season  distance_km  \\\n",
       "1017              0            1                  0   127.606239   \n",
       "2724              0            1                  0   110.308921   \n",
       "2726              0            1                  0    21.787261   \n",
       "\n",
       "      time_since_last_trans  ...      long  city_pop        dob  merch_lat  \\\n",
       "1017               0.000000  ... -108.8964      1645 1986-02-17  43.974711   \n",
       "2724              19.961667  ... -108.8964      1645 1986-02-17  42.018766   \n",
       "2726               0.044167  ... -108.8964      1645 1986-02-17  42.961335   \n",
       "\n",
       "      merch_long  trans_dayofweek is_night  usual_lat  usual_long  \\\n",
       "1017 -109.741904                1        0  43.974711 -109.741904   \n",
       "2724 -109.044172                2        0  43.974711 -109.741904   \n",
       "2726 -109.157564                2        0  42.996738 -109.393038   \n",
       "\n",
       "      distance_from_usual_km  \n",
       "1017                0.000000  \n",
       "2724              224.769219  \n",
       "2726               19.556262  \n",
       "\n",
       "[3 rows x 28 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop high-cardinality columns to reduce memory & overfitting\n",
    "high_cardinality_cols = ['merchant', 'cc_num']  # add any others if needed\n",
    "df = df.drop(columns=high_cardinality_cols, errors='ignore')\n",
    "\n",
    "# Reorder columns for clarity (focus on key ones first)\n",
    "important_cols = ['trans_date_trans_time', 'category', 'amt', 'log_amt',\n",
    "                  'trans_hour', 'is_late_night', 'trans_month', 'is_holiday_season',\n",
    "                  'distance_km', 'time_since_last_trans', 'count_30_days', 'count_7_days', \n",
    "                  'count_1_day', 'age_at_trans', 'age_bin', 'is_fraud']\n",
    "\n",
    "df_processed = df[important_cols + [col for col in df.columns if col not in important_cols]]\n",
    "\n",
    "# Save the processed DataFrame to CSV\n",
    "processed_file = os.path.join(PROCESSED_DATA_PATH, 'train_processed.csv')\n",
    "df_processed.to_csv(processed_file, index=False)\n",
    "print(\"Processed data saved to:\", processed_file)\n",
    "\n",
    "# Final verification\n",
    "print(\"\\nProcessed shape:\", df_processed.shape)\n",
    "df_processed.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2aec450-baeb-4854-947f-dba055f6456c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
